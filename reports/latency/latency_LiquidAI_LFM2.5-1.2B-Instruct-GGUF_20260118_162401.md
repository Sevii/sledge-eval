# Latency Benchmark Report: LiquidAI/LFM2.5-1.2B-Instruct-GGUF

**Target Latency:** 100.0ms
**Timestamp:** 2026-01-18T16:24:01.572261
**Server:** http://localhost:8080

## Executive Summary

- **Target Hit Rate:** ⚠️ 0.0% (0/17 tests)
- **Average Latency:** 525.2ms (target: 100.0ms)
- **Latency Range:** 395.3ms - 637.5ms
- **Tool Call Accuracy:** 52.9%

## Latency Distribution

| Percentile | Latency (ms) |
|------------|--------------|
| P50 | 508.4 |
| P95 | 613.4 |
| P99 | 632.7 |

## Results by Optimization Category

| Category | Tests | Avg Latency | Target Rate | Improvement |
|----------|-------|-------------|-------------|-------------|
| baseline | 5 | 542.5ms | ❌ 0.0% | - |
| combined_optimizations | 2 | 482.4ms | ❌ 0.0% | +11.1% |
| multi_tool | 1 | 529.2ms | ❌ 0.0% | +2.4% |
| multi_tool_short | 1 | 524.8ms | ❌ 0.0% | +3.3% |
| prefix_injection | 3 | 490.4ms | ❌ 0.0% | +9.6% |
| short_fields | 3 | 507.3ms | ❌ 0.0% | +6.5% |
| speculative_friendly | 2 | 602.2ms | ❌ 0.0% | -11.0% |

## Individual Test Results

| Test ID | Category | Latency | Target | Tool Correct |
|---------|----------|---------|--------|--------------|
| latency_baseline_001 | baseline | 590.5ms | ❌ | ✅ |
| latency_baseline_002 | baseline | 486.5ms | ❌ | ✅ |
| latency_baseline_003 | baseline | 602.5ms | ❌ | ✅ |
| latency_baseline_004 | baseline | 637.5ms | ❌ | ✅ |
| latency_baseline_005 | baseline | 395.3ms | ❌ | ✅ |
| latency_prefix_001 | prefix_injection | 507.3ms | ❌ | ✅ |
| latency_prefix_002 | prefix_injection | 455.6ms | ❌ | ❌ |
| latency_prefix_003 | prefix_injection | 508.4ms | ❌ | ✅ |
| latency_short_001 | short_fields | 461.9ms | ❌ | ❌ |
| latency_short_002 | short_fields | 465.6ms | ❌ | ❌ |
| latency_short_003 | short_fields | 594.5ms | ❌ | ❌ |
| latency_combined_001 | combined_optimizations | 488.5ms | ❌ | ❌ |
| latency_combined_002 | combined_optimizations | 476.3ms | ❌ | ❌ |
| latency_multi_001 | multi_tool | 529.2ms | ❌ | ❌ |
| latency_multi_short_001 | multi_tool_short | 524.8ms | ❌ | ❌ |
| latency_speculative_001 | speculative_friendly | 597.0ms | ❌ | ✅ |
| latency_speculative_002 | speculative_friendly | 607.4ms | ❌ | ✅ |

## Optimization Recommendations

Current average latency is **425.2ms above target**. Consider:

1. **Increase Throughput:** Switch to TensorRT-LLM or SGLang
2. **Enable Speculative Decoding:** Use a draft model for predictable tool calls
3. **Use Prefix Injection:** Saves ~34.8ms per call
4. **Use Short Field Names:** Saves ~17.9ms per call
5. **Quantization:** Try AWQ/GPTQ Int4 to increase memory bandwidth

---
*Generated by Sledge Eval Latency Benchmark*